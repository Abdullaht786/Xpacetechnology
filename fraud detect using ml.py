# -*- coding: utf-8 -*-
"""Proyecto Final IA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/proyecto-final-ia-00693d3e-43af-403f-85d4-0d0d3a1bea1e.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241117/auto/storage/goog4_request%26X-Goog-Date%3D20241117T082350Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D92ab489c52bb53d2039ae7bf998df01e1bea6407ad4fb746ec4659fe91a0ffef1e8ad05a698bd084c94066f53f86af81a39c4b124db012ade439cdaf67490f1b1d9519ef1a37538409e1136e3b6729a181f43546b7b7a30e5a4ac00566e89d7e1c13c67c9ec8985fba484f7bfe1182b9dcd2a594e1a24d9313c29d66828027d86a4d9e5afdc356d0b89889689ebd504a854e2fe82101a8dc3cb92fc4ed1e15f42fefd1e25577f1073c175eb25805f92317b580d200290064d1f782aad5e7f862eebe53e444884960051d2e6ee1328502c7006819f37a694e9f4d7a404eef28b1937a4b40a8c067870a19d30fd9c6bddd110725e1f3774719e258009c5c7bd256
"""

import pandas as pd           # Para manipulación y análisis de datos
import numpy as np            # Para operaciones numéricas
import matplotlib.pyplot as plt  # Para visualización de datos
import seaborn as sns         # Para visualización de datos
from sklearn.model_selection import train_test_split  # Para dividir el dataset
from sklearn.preprocessing import StandardScaler      # Para normalizar los datos
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.callbacks import EarlyStopping

print("Here we go")

# Cargar el dataset
df = pd.read_csv('/kaggle/input/credit-card-fraud-detection/credit_card_fraud_dataset.csv')  # Leer el dataset

# Verificar las primeras filas del DataFrame
print(df.head())
print(df.info())

# Resumen estadístico
print(df.describe())

# Ver la distribución de la variable objetivo 'IsFraud'
print(df['IsFraud'].value_counts())


sns.countplot(x='IsFraud', data=df)
plt.title('Distribución de Transacciones (Fraudulentas vs No Fraudulentas)')
plt.xlabel('IsFraud')
plt.ylabel('Cantidad de Transacciones')
plt.show()

# Resumen de las variables categóricas
print(df['TransactionType'].value_counts())
print(df['Location'].value_counts())

# Correlación entre Amount e IsFraud
correlation = df[['Amount', 'IsFraud']].corr()
print(correlation)

# Visualización de la correlación
sns.scatterplot(x='Amount', y='IsFraud', data=df)
plt.title('Correlación entre Amount e IsFraud')
plt.xlabel('Amount')
plt.ylabel('IsFraud')
plt.show()

"""Puede verse como no existe una correlación entre cantidad y fraude. Al ser dos variable númericas, podría paracer que exista relación."""

#La fecha puede ser una variable muy útil para detectar patrones en el tiempo, o correlación entre la salida y la entrada (fraudes en vacaciones, fechas señaladas)
#Vamos a estudiar si es realmente relevante

#Preparación y conversión de datos
# Convertir TransactionDate a datetime
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'])
# Extraer características
df['Year'] = df['TransactionDate'].dt.year
df['Month'] = df['TransactionDate'].dt.month
df['Day'] = df['TransactionDate'].dt.day
df['Hour'] = df['TransactionDate'].dt.hour
df['DayOfWeek'] = df['TransactionDate'].dt.dayofweek  # 0=Monday, 6=Sunday

# (Opcional) Eliminar la columna original si no se necesita
df.drop(columns=['TransactionDate'], inplace=True)

# Visualización por Mes
sns.countplot(x='Month', hue='IsFraud', data=df)
plt.title('Transacciones por Mes (Fraudulentas vs No Fraudulentas)')
plt.xlabel('Mes')
plt.ylabel('Cantidad de Transacciones')
plt.show()

# Visualización por Día de la Semana
sns.countplot(x='DayOfWeek', hue='IsFraud', data=df)
plt.title('Transacciones por Día de la Semana (Fraudulentas vs No Fraudulentas)')
plt.xlabel('Día de la Semana')
plt.ylabel('Cantidad de Transacciones')
plt.show()

# Visualización por Día de la Semana
sns.countplot(x='Hour', hue='IsFraud', data=df)
plt.title('Transacciones por Hora del día (Fraudulentas vs No Fraudulentas)')
plt.xlabel('Hora del día')
plt.ylabel('Cantidad de Transacciones')
plt.show()

df.head()

#Se observa que si que parece haber una variación significativa en el número de fraudes segun la hora del día.
#Por ello se realiza un análisis en detalle

# Contar transacciones por mes y fraude
hour_counts = df.groupby(['Hour', 'IsFraud']).size().unstack(fill_value=0)

# Calcular porcentajes
hour_percentages = hour_counts.div(hour_counts.sum(axis=1), axis=0) * 100

# Imprimir resultados
print("Transacciones por Hora:")
for hour in range(1, 13):
    fraud_count = hour_counts.loc[hour, 1]
    non_fraud_count = hour_counts.loc[hour, 0]
    total_count = fraud_count + non_fraud_count
    print(f"Hora {hour}: {total_count} total, {fraud_count} fraudulentas ({hour_percentages.loc[hour, 1]:.2f}%)")

# Asignar valores numéricos a las columnas 'Location' y 'TransactionType'
df['Location'], location_unique = pd.factorize(df['Location'])
df['TransactionType'], transaction_type_unique = pd.factorize(df['TransactionType'])

# Verificar los resultados
print("Valores únicos para 'Location':", location_unique)
print("Valores únicos para 'TransactionType':", transaction_type_unique)
print("\nDataFrame actualizado:")
print(df.head(100))

"""La variación es de hasta casi un 30% mayor en algunos casos, por lo que se decide que la hora si es importante, pero no el resto de datos"""

#elimino datos no interesantes
#df = df.drop(columns=['Year', 'Month', 'Day', 'DayOfWeek'])
#tambien el transactionID ya que solo sirve para identificar
df['AmountFactor'] = (df['Location']+1)*0.5
df = df.drop(columns=['TransactionID'])
df.head()

"""Entrenamiento del modelo"""

# Separar características y variable objetivo
X = df.drop('IsFraud', axis=1)  # Características
y = df['IsFraud']                # Variable objetivo

# Dividir en conjunto de entrenamiento y conjunto de prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Verificar las dimensiones de los conjuntos
print("Conjunto de Entrenamiento:", X_train.shape)
print("Conjunto de Prueba:", X_test.shape)


#Dado que dataset está desbalanceado, se aplica un peso a la clase minoritaria
# Calcular el peso para la clase minoritaria
from sklearn.utils import class_weight

# Calcular los pesos
class_weights = class_weight.compute_class_weight('balanced', classes=[0, 1], y=y_train)
weight_dict = {0: class_weights[0], 1: class_weights[1]}

print("Pesos para las clases:", weight_dict)

# Importar las librerías necesarias
import xgboost as xgb
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split

# Separar características y variable objetivo
X = df.drop('IsFraud', axis=1)  # Características
y = df['IsFraud']                # Variable objetivo

# Dividir en conjunto de entrenamiento y conjunto de prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42, stratify=y)

# Crear el clasificador de XGBoost
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Entrenar el modelo
xgb_model.fit(X_train, y_train)

# Realizar predicciones
y_pred = xgb_model.predict(X_test)

# Calcular las métricas
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Imprimir la matriz de confusión
print("Matriz de Confusión:")
print(confusion_matrix(y_test, y_pred))

# Imprimir el reporte de clasificación
print("\nReporte de Clasificación:")
print(classification_report(y_test, y_pred))

# Imprimir las métricas
print("\nMétricas del Modelo:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# Importar las librerías necesarias
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score
import xgboost as xgb

# Separar características y variable objetivo
X = df.drop('IsFraud', axis=1)  # Características
y = df['IsFraud']                # Variable objetivo

# Aplicar SMOTE
smote = SMOTE(random_state=80)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Dividir el conjunto balanceado en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=42, stratify=y_resampled)

# Crear el clasificador de XGBoost
xgb_model = xgb.XGBClassifier(use_label_encoder=True, eval_metric='logloss', random_state=42)

# Entrenar el modelo
xgb_model.fit(X_train, y_train)

# Realizar predicciones
y_pred = xgb_model.predict(X_test)

# Calcular las métricas
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Imprimir la matriz de confusión
print("Matriz de Confusión:")
print(confusion_matrix(y_test, y_pred))

# Imprimir el reporte de clasificación
print("\nReporte de Clasificación:")
print(classification_report(y_test, y_pred))

# Imprimir las métricas
print("\nMétricas del Modelo:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

"""Se desarrolla a continuación la solución al problema utilizando Deep Learning"""

X = df.drop('IsFraud', axis=1)  # características
y = df['IsFraud']  # etiqueta

# Dividir en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Aplicar SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train_resampled)
X_test = scaler.transform(X_test)
y_train = y_train_resampled

# Crear el modelo
from keras.layers import Dropout
from keras.layers import Dense
model = keras.Sequential()
model.add(Dense(128, activation='relu', input_dim=X_train_resampled.shape[1]))
model.add(Dropout(0.5))  # Regularización por Dropout
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))  # Otra capa de Dropout
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compilar el modelo
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=5)

# Paso 3: Entrenar el modelo
model.fit(X_train_resampled, y_train_resampled,
          validation_split=0.2,
          epochs=20,
          batch_size=32,
          callbacks=[early_stopping])

# Evaluar el modelo
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Accuracy: {accuracy:.4f}')

y_pred = (model.predict(X_test) > 0.5).astype("int32")
# Imprimir la matriz de confusión
print("Matriz de Confusión:")
print(confusion_matrix(y_test, y_pred))

# Imprimir el reporte de clasificación
print("\nReporte de Clasificación:")
print(classification_report(y_test, y_pred))

# Imprimir las métricas
print("\nMétricas del Modelo:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")